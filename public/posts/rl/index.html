<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reinforcement learning (without neural networks) | Mayur</title>
<meta name=keywords content="Reinforcement learning,Machine Learning,AI"><meta name=description content="A detailed introduction to reinforcement learning fundamentals without neural networks"><meta name=author content="Mayur Hulke"><link rel=canonical href=http://localhost:1313/posts/rl/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/rl/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/rl/"><meta property="og:site_name" content="Mayur"><meta property="og:title" content="Reinforcement learning (without neural networks)"><meta property="og:description" content="A detailed introduction to reinforcement learning fundamentals without neural networks"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-20T11:30:03+00:00"><meta property="article:modified_time" content="2024-04-20T11:30:03+00:00"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement learning (without neural networks)"><meta name=twitter:description content="A detailed introduction to reinforcement learning fundamentals without neural networks"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Reinforcement learning (without neural networks)","item":"http://localhost:1313/posts/rl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reinforcement learning (without neural networks)","name":"Reinforcement learning (without neural networks)","description":"A detailed introduction to reinforcement learning fundamentals without neural networks","keywords":["Reinforcement learning","Machine Learning","AI"],"articleBody":" Estimated Reading Time: 15 min\nRL Framework: The Problem In this post, you’ll learn how to specify a real-world problem as a Markov Decision Process (MDP), so that it can be solved with reinforcement learning.\nThe Setting Let’s kick things off with a simple example. Imagine you’re playing a video game where you’re navigating through a maze. At first, you might bump into walls or take wrong turns, but over time, you learn the layout, figure out where the traps are, and start making smarter moves to reach the end faster and with higher scores. This process of learning through experience, where every choice brings you closer to mastering the maze, is a lot like what we see in reinforcement learning (RL), but on a broader and more complex scale.\nIn RL, the concept is pretty straightforward: we have an agent, which could be anything from a robot to a software program, trying to figure out the best way to accomplish a task. The agent explores its environment, making decisions and learning from the outcomes of those decisions. The feedback comes in the form of rewards - positive for good decisions that move it closer to its goal, and negative for decisions that don’t.\nLet’s break this down with a more relatable example. Consider a thermostat programmed to keep your home at a comfortable temperature. Initially, the thermostat might not know the best settings to use during different times of the day or in varying weather conditions. However, as it ’experiences’ how different settings affect the temperature and receives feedback (like adjustments made by the occupants), it learns and adjusts its actions to maintain the desired comfort level more efficiently.\nThe learning journey involves the agent interacting with its environment in a cycle of actions, observations, and feedback. The agent observes its current state (like the thermostat noting the current temperature), makes a decision (adjusting the temperature setting), and then receives feedback (the house reaching the desired temperature). This feedback is crucial because it tells the agent how well it’s doing and guides its future decisions.\nIn the context of RL, we often simplify the scenario by assuming the agent has a clear view of its environment’s state at each step. This helps us focus on understanding how the agent makes decisions and learns from them. We use terms such as ‘state’, ‘action’, and ‘reward’ to describe this process in a precise manner.\nThis is essentially reinforcement learning (RL), which doesn’t change much whether we’re talking about thermostat , self-driving cars, robots, or any other RL agents. Essentially, RL involves an agent (like our thermostat) figuring out how to act in its world. We look at time in steps and start with the agent seeing its world. From what it sees, the agent decides what to do next. After the action, it finds itself in a new situation and gets a reward, which tells it if the action was a good choice. This cycle of observing, acting, and getting rewards keeps going, with the agent always aiming to pick actions that bring the best rewards over time.\nIn simple terms, we often assume the agent can see everything it needs to make the best choices, although that’s not always true in real life. For our discussions, let’s stick with this idea because it makes the math easier. We’ll say the agent knows the state of its world at every step. Starting from step zero, the agent sees the world state (let’s call this $S_0$), chooses an action ($A_0$),\nand based on that, the world changes to a new state ($S_1$), and the agent gets a reward ($R_1$).\nThe agent then chooses an action, A1.\nAt timestep two, This process keeps repeating, with the agent continuously adjusting its actions based on the world’s state and the rewards it receives. This interaction is manifest as a sequence of states, actions, and rewards.\n$$ S_0, A_0, \\underline{\\mathbf{R_1}}, S_1, A_1, \\underline{\\mathbf{R_2}}, S_2, A_2, \\underline{\\mathbf{R_3}}, S_3, A_3, \\underline{\\mathbf{R_4}}, \\ldots $$\nOur goal in RL is for the agent to maximize its total rewards over time, which it can only do by interacting with its environment and learning from it. The agent has to follow the world’s rules, but by doing so, it learns which actions lead to the best outcomes. This is the core of what we’ll explore in this post. But remember, we’re applying mathematical models to real-world problems. If you’re thinking of solving a problem with RL, you’ll need to define the states, actions, and rewards, and figure out the rules of the world for your specific case. Throughout this post, we’ll explore various examples that show how to set up and solve problems using RL, giving you the tools and understanding you need to tackle challenges that can benefit from this kind of learning approach.\nEpisodic vs. Continuing Tasks Let’s explore several real-world scenarios that conclude with a well-defined endpoint. For example, if we’re training an agent to play a game, the session ends once the agent either wins or loses. Similarly, if we’re conducting a simulation to train a car/robot to drive, the session concludes if the car/robot crashes. Not all tasks in reinforcement learning are like this; however, those that are, are termed episodic tasks. Here, an episode encompasses the entire sequence of interactions from start to finish.\nIn an episodic task within reinforcement learning, the interaction sequence can be represented as follows:\n$$ S_0, A_0, R_1, S_1, A_1, \\ldots, R_T, S_T $$\nwhere:\n$( S_t )$ represents the state at time step $( t )$, $( A_t )$ represents the action taken at time step $( t )$, $( R_{t+1} )$ represents the reward received after taking action $( A_t )$, $( T )$ represents the final time step of the episode. At the end of an episode, the agent evaluates its total reward to assess its performance. It then starts over, effectively reborn with the knowledge from its previous experiences, allowing it to make progressively better decisions. This iterative learning is evident in coding tasks. As agents become more familiar with their environment, they will develop strategies that maximize their cumulative rewards. In the context of a gaming agent, this means achieving higher scores.\nEpisodic tasks, therefore, are defined by their clear endpoints. We will also discuss ongoing tasks, known as continuing tasks, where there is no end. An example would be an algorithm that continuously buys and sells stocks based on market conditions, best modeled as a continuing task where the agent operates indefinitely. These agents must learn to optimize their actions continuously while interacting with their environment.\nFor a continuing task within reinforcement learning, the interaction sequence is unbounded and can be represented as follows:\n$$ S_0, A_0, R_1, S_1, A_1, \\ldots $$\nwhere:\n$( S_t )$ denotes the state at time step $( t )$, $( A_t )$ denotes the action taken at time step $( t )$, $( R_{t+1} )$ denotes the reward received after taking action $( A_t )$, and the sequence continues indefinitely without a predefined ending point. The strategies for these tasks are more complex and will be introduced later in this blog. Now, let’s dive deeper into the concept of rewards in these settings.\nThe Reward Hypothesis We’ve talked about the wide-ranging uses of Reinforcement Learning, each defined by its unique agent and environment, where every agent is driven by a goal. These goals are as varied as teaching a car to navigate autonomously or training an agent to excel at Atari games. It’s fascinating that such disparate objectives can all be approached using the same underlying principles.\nUp to this point, we’ve examined the concept of reward through a simple analogy: navigating a maze in a video game. In this scenario, the layout of the maze represents the state, your decisions on which turns to take are the actions, and the reward is the score or feedback you receive from the game, such as points or advancing to the next level. Just like a reinforcement learning agent, you aim to maximize this reward by learning from each interaction, which reflects the process of trial and error and gradual improvement similar to training within a Reinforcement Learning framework.\nHowever, the Reinforcement Learning Framework generalizes this to have all agents define their objectives in terms of maximizing expected cumulative rewards. But what does ‘reward’ signify for a robot learning to walk? Could the environment act as a coach, giving feedback on the robot’s technique, rewarding good form? Yet, the reward in this context might seem subjective and unscientific. What criteria determine a ‘good’ walk versus a ‘bad’ one, and how do we quantify this in our models?\nTo address these concerns, we must understand that the term ‘reward’ and the concept of Reinforcement Learning are borrowed from behavioral science. It signifies a stimulus given right after a behavior to increase the likelihood of that behavior’s future occurrence.\nThe fact that Reinforcement Learning shares its name with a behavioral science term is deliberate, underscoring the foundational hypothesis in Reinforcement Learning:\nWe can always express an agent’s goal in terms of maximizing expected cumulative rewards. This is known as the “Reward Hypothesis.”\nIf the application of this hypothesis to complex or abstract tasks feels strange or challenging, you’re not alone.I’ll aim to further clarify and justify this fundamental concept.\n","wordCount":"1556","inLanguage":"en","datePublished":"2024-04-20T11:30:03Z","dateModified":"2024-04-20T11:30:03Z","author":{"@type":"Person","name":"Mayur Hulke"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/rl/"},"publisher":{"@type":"Organization","name":"Mayur","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Mayur (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Mayur</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Reinforcement learning (without neural networks)</h1><div class=post-description>A detailed introduction to reinforcement learning fundamentals without neural networks</div><div class=post-meta><span title='2024-04-20 11:30:03 +0000 +0000'>April 20, 2024</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1556 words&nbsp;·&nbsp;Mayur Hulke</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#rl-framework-the-problem>RL Framework: The Problem</a><ul><li><a href=#the-setting>The Setting</a></li><li><a href=#episodic-vs-continuing-tasks>Episodic vs. Continuing Tasks</a></li><li><a href=#the-reward-hypothesis>The Reward Hypothesis</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p><img alt="Status: Currently Writing" loading=lazy src=https://img.shields.io/badge/Status-Currently%20Writing-blue>
<strong>Estimated Reading Time: 15 min</strong></p><h2 id=rl-framework-the-problem>RL Framework: The Problem<a hidden class=anchor aria-hidden=true href=#rl-framework-the-problem>#</a></h2><p>In this post, you&rsquo;ll learn how to specify a real-world problem as a Markov Decision Process (MDP), so that it can be solved with reinforcement learning.</p><h3 id=the-setting>The Setting<a hidden class=anchor aria-hidden=true href=#the-setting>#</a></h3><p>Let&rsquo;s kick things off with a simple example. Imagine you&rsquo;re playing a video game where you&rsquo;re navigating through a maze. At first, you might bump into walls or take wrong turns, but over time, you learn the layout, figure out where the traps are, and start making smarter moves to reach the end faster and with higher scores. This process of learning through experience, where every choice brings you closer to mastering the maze, is a lot like what we see in reinforcement learning (RL), but on a broader and more complex scale.</p><p><img alt="The Setting" loading=lazy src=/images/the_setting.png></p><p>In RL, the concept is pretty straightforward: we have an agent, which could be anything from a robot to a software program, trying to figure out the best way to accomplish a task. The agent explores its environment, making decisions and learning from the outcomes of those decisions. The feedback comes in the form of rewards - positive for good decisions that move it closer to its goal, and negative for decisions that don&rsquo;t.</p><p>Let&rsquo;s break this down with a more relatable example. Consider a thermostat programmed to keep your home at a comfortable temperature. Initially, the thermostat might not know the best settings to use during different times of the day or in varying weather conditions. However, as it &rsquo;experiences&rsquo; how different settings affect the temperature and receives feedback (like adjustments made by the occupants), it learns and adjusts its actions to maintain the desired comfort level more efficiently.</p><p>The learning journey involves the agent interacting with its environment in a cycle of actions, observations, and feedback. The agent observes its current state (like the thermostat noting the current temperature), makes a decision (adjusting the temperature setting), and then receives feedback (the house reaching the desired temperature). This feedback is crucial because it tells the agent how well it&rsquo;s doing and guides its future decisions.</p><p>In the context of RL, we often simplify the scenario by assuming the agent has a clear view of its environment&rsquo;s state at each step. This helps us focus on understanding how the agent makes decisions and learns from them. We use terms such as &lsquo;<strong>state</strong>&rsquo;, &lsquo;<strong>action</strong>&rsquo;, and &lsquo;<strong>reward</strong>&rsquo; to describe this process in a precise manner.</p><p>This is essentially reinforcement learning (RL), which doesn&rsquo;t change much whether we&rsquo;re talking about thermostat , self-driving cars, robots, or any other RL agents. Essentially, RL involves an agent (like our thermostat) figuring out how to act in its world. We look at time in steps and start with the agent seeing its world. From what it sees, the agent decides what to do next. After the action, it finds itself in a new situation and gets a reward, which tells it if the action was a good choice. This cycle of observing, acting, and getting rewards keeps going, with the agent always aiming to pick actions that bring the best rewards over time.</p><p>In simple terms, we often assume the agent can see everything it needs to make the best choices, although that&rsquo;s not always true in real life. For our discussions, let&rsquo;s stick with this idea because it makes the <strong>math easier</strong>. We&rsquo;ll say the agent knows the state of its world at every step. Starting from step zero, the agent sees the world state (let&rsquo;s call this $S_0$), chooses an action ($A_0$),</p><p><img alt="The Setting" loading=lazy src=/images/3.png></p><p>and based on that, the world changes to a new state ($S_1$), and the agent gets a reward ($R_1$).</p><p><img alt="The Setting" loading=lazy src=/images/4.png></p><p>The agent then chooses an action, A1.</p><p><img alt="The Setting" loading=lazy src=/images/5.png></p><p>At timestep two, This process keeps repeating, with the agent continuously adjusting its actions based on the world&rsquo;s state and the rewards it receives.
This interaction is manifest as a sequence of <code>states</code>, <code>actions</code>, and <code>rewards</code>.</p><p><img alt="The Setting" loading=lazy src=/images/6.png></p><p>$$
S_0, A_0, \underline{\mathbf{R_1}}, S_1, A_1, \underline{\mathbf{R_2}}, S_2, A_2, \underline{\mathbf{R_3}}, S_3, A_3, \underline{\mathbf{R_4}}, \ldots
$$</p><p>Our goal in RL is for the agent to <strong>maximize its total rewards</strong> over time, which it can only do by interacting with its environment and learning from it. The agent has to follow the world&rsquo;s rules, but by doing so, it learns which actions lead to the best outcomes. This is the core of what we&rsquo;ll explore in this post. But remember, we&rsquo;re applying mathematical models to real-world problems. If you&rsquo;re thinking of solving a problem with RL, you&rsquo;ll need to define the <strong>states</strong>, <strong>actions</strong>, and <strong>rewards</strong>, and figure out the rules of the world for your specific case. Throughout this post, we&rsquo;ll explore various examples that show how to set up and solve problems using RL, giving you the tools and understanding you need to tackle challenges that can benefit from this kind of learning approach.</p><h3 id=episodic-vs-continuing-tasks>Episodic vs. Continuing Tasks<a hidden class=anchor aria-hidden=true href=#episodic-vs-continuing-tasks>#</a></h3><p>Let&rsquo;s explore several real-world scenarios that conclude with a well-defined endpoint. For example, if we&rsquo;re training an agent to play a game, the session ends once the agent either wins or loses. Similarly, if we&rsquo;re conducting a simulation to train a car/robot to drive, the session concludes if the car/robot crashes. Not all tasks in reinforcement learning are like this; however, those that are, are termed <strong>episodic tasks</strong>. Here, an episode encompasses the entire sequence of interactions from start to finish.</p><p>In an episodic task within reinforcement learning, the interaction sequence can be represented as follows:</p><p>$$
S_0, A_0, R_1, S_1, A_1, \ldots, R_T, S_T
$$</p><p>where:</p><ul><li>$( S_t )$ represents the state at time step $( t )$,</li><li>$( A_t )$ represents the action taken at time step $( t )$,</li><li>$( R_{t+1} )$ represents the reward received after taking action $( A_t )$,</li><li>$( T )$ represents the final time step of the episode.</li></ul><p>At the end of an episode, the agent evaluates its total reward to assess its performance. It then starts over, effectively reborn with the knowledge from its previous experiences, allowing it to make progressively better decisions. This iterative learning is evident in coding tasks. As agents become more familiar with their environment, they will develop strategies that maximize their cumulative rewards. In the context of a gaming agent, this means achieving higher scores.</p><p><strong>Episodic tasks, therefore, are defined by their clear endpoints</strong>. We will also discuss ongoing tasks, known as <strong>continuing tasks, where there is no end</strong>. An example would be an algorithm that continuously buys and sells stocks based on market conditions, best modeled as a continuing task where the agent operates indefinitely. These agents must learn to optimize their actions continuously while interacting with their environment.</p><p>For a continuing task within reinforcement learning, the interaction sequence is unbounded and can be represented as follows:</p><p>$$
S_0, A_0, R_1, S_1, A_1, \ldots
$$</p><p>where:</p><ul><li>$( S_t )$ denotes the state at time step $( t )$,</li><li>$( A_t )$ denotes the action taken at time step $( t )$,</li><li>$( R_{t+1} )$ denotes the reward received after taking action $( A_t )$,</li><li>and the sequence continues indefinitely without a predefined ending point.</li></ul><p>The strategies for these tasks are more complex and will be introduced later in this blog. Now, let&rsquo;s dive deeper into the concept of <strong>rewards</strong> in these settings.</p><h3 id=the-reward-hypothesis>The Reward Hypothesis<a hidden class=anchor aria-hidden=true href=#the-reward-hypothesis>#</a></h3><p>We&rsquo;ve talked about the wide-ranging uses of Reinforcement Learning, each defined by its unique agent and environment, where every agent is driven by a goal. These goals are as varied as teaching a car to navigate autonomously or training an agent to excel at Atari games. It&rsquo;s fascinating that such disparate objectives can all be approached using the same underlying principles.</p><p>Up to this point, we&rsquo;ve examined the concept of reward through a simple analogy: navigating a maze in a video game. In this scenario, the layout of the maze represents the <strong>state</strong>, your decisions on which turns to take are the <strong>actions</strong>, and the <strong>reward</strong> is the score or feedback you receive from the game, such as points or advancing to the next level. Just like a reinforcement learning agent, you aim to maximize this reward by learning from each interaction, which reflects the process of trial and error and gradual improvement similar to training within a Reinforcement Learning framework.</p><p><img alt="The Reward Hypothesis" loading=lazy src=/images/agent.gif></p><p>However, the Reinforcement Learning Framework generalizes this to have all agents define their objectives in terms of maximizing expected cumulative rewards. But what does <strong>&lsquo;reward&rsquo;</strong> signify for a robot learning to walk? Could the environment act as a coach, giving feedback on the robot&rsquo;s technique, rewarding good form? Yet, the reward in this context might seem subjective and unscientific. <strong>What criteria determine a &lsquo;good&rsquo; walk versus a &lsquo;bad&rsquo; one, and how do we quantify this in our models?</strong></p><p>To address these concerns, we must understand that the term &lsquo;reward&rsquo; and the concept of Reinforcement Learning are borrowed from behavioral science. It signifies a stimulus given right after a behavior to increase the likelihood of that behavior&rsquo;s future occurrence.</p><p>The fact that Reinforcement Learning shares its name with a behavioral science term is deliberate, underscoring the foundational hypothesis in Reinforcement Learning:</p><p><strong>We can always express an agent&rsquo;s goal in terms of maximizing expected cumulative rewards. This is known as the <strong>&ldquo;Reward Hypothesis.&rdquo;</strong></strong></p><p>If the application of this hypothesis to complex or abstract tasks feels strange or challenging, you&rsquo;re not alone.I&rsquo;ll aim to further clarify and justify this fundamental concept.</p><p><img alt="Status: Currently Writing" loading=lazy src=https://img.shields.io/badge/Status-Currently%20Writing-blue></p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=http://localhost:1313/tags/machine-learning/>Machine Learning</a></li><li><a href=http://localhost:1313/tags/ai/>AI</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement learning (without neural networks) on x" href="https://x.com/intent/tweet/?text=Reinforcement%20learning%20%28without%20neural%20networks%29&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frl%2f&amp;hashtags=Reinforcementlearning%2cMachineLearning%2cAI"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement learning (without neural networks) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frl%2f&amp;title=Reinforcement%20learning%20%28without%20neural%20networks%29&amp;summary=Reinforcement%20learning%20%28without%20neural%20networks%29&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2frl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement learning (without neural networks) on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2frl%2f&title=Reinforcement%20learning%20%28without%20neural%20networks%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement learning (without neural networks) on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2frl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement learning (without neural networks) on whatsapp" href="https://api.whatsapp.com/send?text=Reinforcement%20learning%20%28without%20neural%20networks%29%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2frl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement learning (without neural networks) on telegram" href="https://telegram.me/share/url?text=Reinforcement%20learning%20%28without%20neural%20networks%29&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frl%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reinforcement learning (without neural networks) on ycombinator" href="https://news.ycombinator.com/submitlink?t=Reinforcement%20learning%20%28without%20neural%20networks%29&u=http%3a%2f%2flocalhost%3a1313%2fposts%2frl%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Mayur</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>