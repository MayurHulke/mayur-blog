<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reinforcement learning (without neural networks) | Mayur</title>
<meta name=keywords content="Reinforcement learning,Machine Learning,AI"><meta name=description content="A detailed introduction to reinforcement learning fundamentals without neural networks"><meta name=author content="Mayur Hulke"><link rel=canonical href=http://localhost:1313/posts/rl/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/rl/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/javascript async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><meta property="og:url" content="http://localhost:1313/posts/rl/"><meta property="og:site_name" content="Mayur"><meta property="og:title" content="Reinforcement learning (without neural networks)"><meta property="og:description" content="A detailed introduction to reinforcement learning fundamentals without neural networks"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-20T11:30:03+00:00"><meta property="article:modified_time" content="2024-04-20T11:30:03+00:00"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement learning (without neural networks)"><meta name=twitter:description content="A detailed introduction to reinforcement learning fundamentals without neural networks"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Reinforcement learning (without neural networks)","item":"http://localhost:1313/posts/rl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reinforcement learning (without neural networks)","name":"Reinforcement learning (without neural networks)","description":"A detailed introduction to reinforcement learning fundamentals without neural networks","keywords":["Reinforcement learning","Machine Learning","AI"],"articleBody":" Estimated Reading Time: 15 min\nRL Framework: The Problem In this post, you’ll learn how to specify a real-world problem as a Markov Decision Process (MDP), so that it can be solved with reinforcement learning.\nThe Setting Let’s kick things off with a simple example. Imagine you’re playing a video game where you’re navigating through a maze. At first, you might bump into walls or take wrong turns, but over time, you learn the layout, figure out where the traps are, and start making smarter moves to reach the end faster and with higher scores. This process of learning through experience, where every choice brings you closer to mastering the maze, is a lot like what we see in reinforcement learning (RL), but on a broader and more complex scale.\nIn RL, the concept is pretty straightforward: we have an agent, which could be anything from a robot to a software program, trying to figure out the best way to accomplish a task. The agent explores its environment, making decisions and learning from the outcomes of those decisions. The feedback comes in the form of rewards - positive for good decisions that move it closer to its goal, and negative for decisions that don’t.\nLet’s break this down with a more relatable example. Consider a thermostat programmed to keep your home at a comfortable temperature. Initially, the thermostat might not know the best settings to use during different times of the day or in varying weather conditions. However, as it ’experiences’ how different settings affect the temperature and receives feedback (like adjustments made by the occupants), it learns and adjusts its actions to maintain the desired comfort level more efficiently.\nThe learning journey involves the agent interacting with its environment in a cycle of actions, observations, and feedback. The agent observes its current state (like the thermostat noting the current temperature), makes a decision (adjusting the temperature setting), and then receives feedback (the house reaching the desired temperature). This feedback is crucial because it tells the agent how well it’s doing and guides its future decisions.\nIn the context of RL, we often simplify the scenario by assuming the agent has a clear view of its environment’s state at each step. This helps us focus on understanding how the agent makes decisions and learns from them. We use terms such as ‘state’, ‘action’, and ‘reward’ to describe this process in a precise manner.\nThis is essentially reinforcement learning (RL), which doesn’t change much whether we’re talking about thermostat , self-driving cars, robots, or any other RL agents. Essentially, RL involves an agent (like our thermostat) figuring out how to act in its world. We look at time in steps and start with the agent seeing its world. From what it sees, the agent decides what to do next. After the action, it finds itself in a new situation and gets a reward, which tells it if the action was a good choice. This cycle of observing, acting, and getting rewards keeps going, with the agent always aiming to pick actions that bring the best rewards over time.\nIn simple terms, we often assume the agent can see everything it needs to make the best choices, although that’s not always true in real life. For our discussions, let’s stick with this idea because it makes the math easier. We’ll say the agent knows the state of its world at every step. Starting from step zero, the agent sees the world state (let’s call this $S_0$), chooses an action ($A_0$),\nand based on that, the world changes to a new state ($S_1$), and the agent gets a reward ($R_1$).\nThe agent then chooses an action, A1.\nAt timestep two, This process keeps repeating, with the agent continuously adjusting its actions based on the world’s state and the rewards it receives. This interaction is manifest as a sequence of states, actions, and rewards.\n$$ S_0, A_0, \\underline{\\mathbf{R_1}}, S_1, A_1, \\underline{\\mathbf{R_2}}, S_2, A_2, \\underline{\\mathbf{R_3}}, S_3, A_3, \\underline{\\mathbf{R_4}}, \\ldots $$\nOur goal in RL is for the agent to maximize its total rewards over time, which it can only do by interacting with its environment and learning from it. The agent has to follow the world’s rules, but by doing so, it learns which actions lead to the best outcomes. This is the core of what we’ll explore in this post. But remember, we’re applying mathematical models to real-world problems. If you’re thinking of solving a problem with RL, you’ll need to define the states, actions, and rewards, and figure out the rules of the world for your specific case. Throughout this post, we’ll explore various examples that show how to set up and solve problems using RL, giving you the tools and understanding you need to tackle challenges that can benefit from this kind of learning approach.\nEpisodic vs. Continuing Tasks Let’s explore several real-world scenarios that conclude with a well-defined endpoint. For example, if we’re training an agent to play a game, the session ends once the agent either wins or loses. Similarly, if we’re conducting a simulation to train a car/robot to drive, the session concludes if the car/robot crashes. Not all tasks in reinforcement learning are like this; however, those that are, are termed episodic tasks. Here, an episode encompasses the entire sequence of interactions from start to finish.\nIn an episodic task within reinforcement learning, the interaction sequence can be represented as follows:\n$$ S_0, A_0, R_1, S_1, A_1, \\ldots, R_T, S_T $$\nwhere:\n$( S_t )$ represents the state at time step $( t )$, $( A_t )$ represents the action taken at time step $( t )$, $( R_{t+1} )$ represents the reward received after taking action $( A_t )$, $( T )$ represents the final time step of the episode. At the end of an episode, the agent evaluates its total reward to assess its performance. It then starts over, effectively reborn with the knowledge from its previous experiences, allowing it to make progressively better decisions. This iterative learning is evident in coding tasks. As agents become more familiar with their environment, they will develop strategies that maximize their cumulative rewards. In the context of a gaming agent, this means achieving higher scores.\nEpisodic tasks, therefore, are defined by their clear endpoints. We will also discuss ongoing tasks, known as continuing tasks, where there is no end. An example would be an algorithm that continuously buys and sells stocks based on market conditions, best modeled as a continuing task where the agent operates indefinitely. These agents must learn to optimize their actions continuously while interacting with their environment.\nFor a continuing task within reinforcement learning, the interaction sequence is unbounded and can be represented as follows:\n$$ S_0, A_0, R_1, S_1, A_1, \\ldots $$\nwhere:\n$( S_t )$ denotes the state at time step $( t )$, $( A_t )$ denotes the action taken at time step $( t )$, $( R_{t+1} )$ denotes the reward received after taking action $( A_t )$, and the sequence continues indefinitely without a predefined ending point. The strategies for these tasks are more complex and will be introduced later in this blog. Now, let’s dive deeper into the concept of rewards in these settings.\nThe Reward Hypothesis We’ve talked about the wide-ranging uses of Reinforcement Learning, each defined by its unique agent and environment, where every agent is driven by a goal. These goals are as varied as teaching a car to navigate autonomously or training an agent to excel at Atari games. It’s fascinating that such disparate objectives can all be approached using the same underlying principles.\nUp to this point, we’ve examined the concept of reward through a simple analogy: navigating a maze in a video game. In this scenario, the layout of the maze represents the state, your decisions on which turns to take are the actions, and the reward is the score or feedback you receive from the game, such as points or advancing to the next level. Just like a reinforcement learning agent, you aim to maximize this reward by learning from each interaction, which reflects the process of trial and error and gradual improvement similar to training within a Reinforcement Learning framework.\nHowever, the Reinforcement Learning Framework generalizes this to have all agents define their objectives in terms of maximizing expected cumulative rewards. But what does ‘reward’ signify for a robot learning to walk? Could the environment act as a coach, giving feedback on the robot’s technique, rewarding good form? Yet, the reward in this context might seem subjective and unscientific. What criteria determine a ‘good’ walk versus a ‘bad’ one, and how do we quantify this in our models?\nTo address these concerns, we must understand that the term ‘reward’ and the concept of Reinforcement Learning are borrowed from behavioral science. It signifies a stimulus given right after a behavior to increase the likelihood of that behavior’s future occurrence.\nThe fact that Reinforcement Learning shares its name with a behavioral science term is deliberate, underscoring the foundational hypothesis in Reinforcement Learning:\nWe can always express an agent’s goal in terms of maximizing expected cumulative rewards. This is known as the “Reward Hypothesis.”\nIf the application of this hypothesis to complex or abstract tasks feels strange or challenging, you’re not alone.I’ll aim to further clarify and justify this fundamental concept.\n","wordCount":"1556","inLanguage":"en","datePublished":"2024-04-20T11:30:03Z","dateModified":"2024-04-20T11:30:03Z","author":{"@type":"Person","name":"Mayur Hulke"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/rl/"},"publisher":{"@type":"Organization","name":"Mayur","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Mayur (Alt + H)">Mayur</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/books/ title=Books><span>Books</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Reinforcement learning (without neural networks)
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentcolor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-description>A detailed introduction to reinforcement learning fundamentals without neural networks</div><div class=post-meta><span title='2024-04-20 11:30:03 +0000 +0000'>April 20, 2024</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1556 words&nbsp;·&nbsp;Mayur Hulke</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#rl-framework-the-problem>RL Framework: The Problem</a><ul><li><a href=#the-setting>The Setting</a></li><li><a href=#episodic-vs-continuing-tasks>Episodic vs. Continuing Tasks</a></li><li><a href=#the-reward-hypothesis>The Reward Hypothesis</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p><img alt="Status: Currently Writing" loading=lazy src=https://img.shields.io/badge/Status-Currently%20Writing-blue>
<strong>Estimated Reading Time: 15 min</strong></p><h2 id=rl-framework-the-problem>RL Framework: The Problem<a hidden class=anchor aria-hidden=true href=#rl-framework-the-problem>#</a></h2><p>In this post, you&rsquo;ll learn how to specify a real-world problem as a Markov Decision Process (MDP), so that it can be solved with reinforcement learning.</p><h3 id=the-setting>The Setting<a hidden class=anchor aria-hidden=true href=#the-setting>#</a></h3><p>Let&rsquo;s kick things off with a simple example. Imagine you&rsquo;re playing a video game where you&rsquo;re navigating through a maze. At first, you might bump into walls or take wrong turns, but over time, you learn the layout, figure out where the traps are, and start making smarter moves to reach the end faster and with higher scores. This process of learning through experience, where every choice brings you closer to mastering the maze, is a lot like what we see in reinforcement learning (RL), but on a broader and more complex scale.</p><p><img alt="The Setting" loading=lazy src=/images/the_setting.png></p><p>In RL, the concept is pretty straightforward: we have an agent, which could be anything from a robot to a software program, trying to figure out the best way to accomplish a task. The agent explores its environment, making decisions and learning from the outcomes of those decisions. The feedback comes in the form of rewards - positive for good decisions that move it closer to its goal, and negative for decisions that don&rsquo;t.</p><p>Let&rsquo;s break this down with a more relatable example. Consider a thermostat programmed to keep your home at a comfortable temperature. Initially, the thermostat might not know the best settings to use during different times of the day or in varying weather conditions. However, as it &rsquo;experiences&rsquo; how different settings affect the temperature and receives feedback (like adjustments made by the occupants), it learns and adjusts its actions to maintain the desired comfort level more efficiently.</p><p>The learning journey involves the agent interacting with its environment in a cycle of actions, observations, and feedback. The agent observes its current state (like the thermostat noting the current temperature), makes a decision (adjusting the temperature setting), and then receives feedback (the house reaching the desired temperature). This feedback is crucial because it tells the agent how well it&rsquo;s doing and guides its future decisions.</p><p>In the context of RL, we often simplify the scenario by assuming the agent has a clear view of its environment&rsquo;s state at each step. This helps us focus on understanding how the agent makes decisions and learns from them. We use terms such as &lsquo;<strong>state</strong>&rsquo;, &lsquo;<strong>action</strong>&rsquo;, and &lsquo;<strong>reward</strong>&rsquo; to describe this process in a precise manner.</p><p>This is essentially reinforcement learning (RL), which doesn&rsquo;t change much whether we&rsquo;re talking about thermostat , self-driving cars, robots, or any other RL agents. Essentially, RL involves an agent (like our thermostat) figuring out how to act in its world. We look at time in steps and start with the agent seeing its world. From what it sees, the agent decides what to do next. After the action, it finds itself in a new situation and gets a reward, which tells it if the action was a good choice. This cycle of observing, acting, and getting rewards keeps going, with the agent always aiming to pick actions that bring the best rewards over time.</p><p>In simple terms, we often assume the agent can see everything it needs to make the best choices, although that&rsquo;s not always true in real life. For our discussions, let&rsquo;s stick with this idea because it makes the <strong>math easier</strong>. We&rsquo;ll say the agent knows the state of its world at every step. Starting from step zero, the agent sees the world state (let&rsquo;s call this $S_0$), chooses an action ($A_0$),</p><p><img alt="The Setting" loading=lazy src=/images/3.png></p><p>and based on that, the world changes to a new state ($S_1$), and the agent gets a reward ($R_1$).</p><p><img alt="The Setting" loading=lazy src=/images/4.png></p><p>The agent then chooses an action, A1.</p><p><img alt="The Setting" loading=lazy src=/images/5.png></p><p>At timestep two, This process keeps repeating, with the agent continuously adjusting its actions based on the world&rsquo;s state and the rewards it receives.
This interaction is manifest as a sequence of <code>states</code>, <code>actions</code>, and <code>rewards</code>.</p><p><img alt="The Setting" loading=lazy src=/images/6.png></p><p>$$
S_0, A_0, \underline{\mathbf{R_1}}, S_1, A_1, \underline{\mathbf{R_2}}, S_2, A_2, \underline{\mathbf{R_3}}, S_3, A_3, \underline{\mathbf{R_4}}, \ldots
$$</p><p>Our goal in RL is for the agent to <strong>maximize its total rewards</strong> over time, which it can only do by interacting with its environment and learning from it. The agent has to follow the world&rsquo;s rules, but by doing so, it learns which actions lead to the best outcomes. This is the core of what we&rsquo;ll explore in this post. But remember, we&rsquo;re applying mathematical models to real-world problems. If you&rsquo;re thinking of solving a problem with RL, you&rsquo;ll need to define the <strong>states</strong>, <strong>actions</strong>, and <strong>rewards</strong>, and figure out the rules of the world for your specific case. Throughout this post, we&rsquo;ll explore various examples that show how to set up and solve problems using RL, giving you the tools and understanding you need to tackle challenges that can benefit from this kind of learning approach.</p><h3 id=episodic-vs-continuing-tasks>Episodic vs. Continuing Tasks<a hidden class=anchor aria-hidden=true href=#episodic-vs-continuing-tasks>#</a></h3><p>Let&rsquo;s explore several real-world scenarios that conclude with a well-defined endpoint. For example, if we&rsquo;re training an agent to play a game, the session ends once the agent either wins or loses. Similarly, if we&rsquo;re conducting a simulation to train a car/robot to drive, the session concludes if the car/robot crashes. Not all tasks in reinforcement learning are like this; however, those that are, are termed <strong>episodic tasks</strong>. Here, an episode encompasses the entire sequence of interactions from start to finish.</p><p>In an episodic task within reinforcement learning, the interaction sequence can be represented as follows:</p><p>$$
S_0, A_0, R_1, S_1, A_1, \ldots, R_T, S_T
$$</p><p>where:</p><ul><li>$( S_t )$ represents the state at time step $( t )$,</li><li>$( A_t )$ represents the action taken at time step $( t )$,</li><li>$( R_{t+1} )$ represents the reward received after taking action $( A_t )$,</li><li>$( T )$ represents the final time step of the episode.</li></ul><p>At the end of an episode, the agent evaluates its total reward to assess its performance. It then starts over, effectively reborn with the knowledge from its previous experiences, allowing it to make progressively better decisions. This iterative learning is evident in coding tasks. As agents become more familiar with their environment, they will develop strategies that maximize their cumulative rewards. In the context of a gaming agent, this means achieving higher scores.</p><p><strong>Episodic tasks, therefore, are defined by their clear endpoints</strong>. We will also discuss ongoing tasks, known as <strong>continuing tasks, where there is no end</strong>. An example would be an algorithm that continuously buys and sells stocks based on market conditions, best modeled as a continuing task where the agent operates indefinitely. These agents must learn to optimize their actions continuously while interacting with their environment.</p><p>For a continuing task within reinforcement learning, the interaction sequence is unbounded and can be represented as follows:</p><p>$$
S_0, A_0, R_1, S_1, A_1, \ldots
$$</p><p>where:</p><ul><li>$( S_t )$ denotes the state at time step $( t )$,</li><li>$( A_t )$ denotes the action taken at time step $( t )$,</li><li>$( R_{t+1} )$ denotes the reward received after taking action $( A_t )$,</li><li>and the sequence continues indefinitely without a predefined ending point.</li></ul><p>The strategies for these tasks are more complex and will be introduced later in this blog. Now, let&rsquo;s dive deeper into the concept of <strong>rewards</strong> in these settings.</p><h3 id=the-reward-hypothesis>The Reward Hypothesis<a hidden class=anchor aria-hidden=true href=#the-reward-hypothesis>#</a></h3><p>We&rsquo;ve talked about the wide-ranging uses of Reinforcement Learning, each defined by its unique agent and environment, where every agent is driven by a goal. These goals are as varied as teaching a car to navigate autonomously or training an agent to excel at Atari games. It&rsquo;s fascinating that such disparate objectives can all be approached using the same underlying principles.</p><p>Up to this point, we&rsquo;ve examined the concept of reward through a simple analogy: navigating a maze in a video game. In this scenario, the layout of the maze represents the <strong>state</strong>, your decisions on which turns to take are the <strong>actions</strong>, and the <strong>reward</strong> is the score or feedback you receive from the game, such as points or advancing to the next level. Just like a reinforcement learning agent, you aim to maximize this reward by learning from each interaction, which reflects the process of trial and error and gradual improvement similar to training within a Reinforcement Learning framework.</p><p><img alt="The Reward Hypothesis" loading=lazy src=/images/agent.gif></p><p>However, the Reinforcement Learning Framework generalizes this to have all agents define their objectives in terms of maximizing expected cumulative rewards. But what does <strong>&lsquo;reward&rsquo;</strong> signify for a robot learning to walk? Could the environment act as a coach, giving feedback on the robot&rsquo;s technique, rewarding good form? Yet, the reward in this context might seem subjective and unscientific. <strong>What criteria determine a &lsquo;good&rsquo; walk versus a &lsquo;bad&rsquo; one, and how do we quantify this in our models?</strong></p><p>To address these concerns, we must understand that the term &lsquo;reward&rsquo; and the concept of Reinforcement Learning are borrowed from behavioral science. It signifies a stimulus given right after a behavior to increase the likelihood of that behavior&rsquo;s future occurrence.</p><p>The fact that Reinforcement Learning shares its name with a behavioral science term is deliberate, underscoring the foundational hypothesis in Reinforcement Learning:</p><p><strong>We can always express an agent&rsquo;s goal in terms of maximizing expected cumulative rewards. This is known as the <strong>&ldquo;Reward Hypothesis.&rdquo;</strong></strong></p><p>If the application of this hypothesis to complex or abstract tasks feels strange or challenging, you&rsquo;re not alone.I&rsquo;ll aim to further clarify and justify this fundamental concept.</p><p><img alt="Status: Currently Writing" loading=lazy src=https://img.shields.io/badge/Status-Currently%20Writing-blue></p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=http://localhost:1313/tags/machine-learning/>Machine Learning</a></li><li><a href=http://localhost:1313/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/why-love-debugging/><span class=title>« Prev</span><br><span>Why I Learned to Love Debugging (And Why You Should Too)</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Mayur</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><div class=social-icons align=center><a href=https://www.youtube.com/@LetsTalkWithRobots target=_blank rel="noopener noreferrer me" title=Youtube><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22.54 6.42a2.78 2.78.0 00-1.94-2C18.88 4 12 4 12 4s-6.88.0-8.6.46a2.78 2.78.0 00-1.94 2A29 29 0 001 11.75a29 29 0 00.46 5.33A2.78 2.78.0 003.4 19c1.72.46 8.6.46 8.6.46s6.88.0 8.6-.46a2.78 2.78.0 001.94-2 29 29 0 00.46-5.25 29 29 0 00-.46-5.33z"/><polygon points="9.75 15.02 15.5 11.75 9.75 8.48 9.75 15.02"/></svg>
</a><a href=https://twitter.com/LetstalkRobots target=_blank rel="noopener noreferrer me" title=X><svg viewBox="0 0 24 24" fill="currentcolor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
</a><a href=https://www.instagram.com/themayurhulk target=_blank rel="noopener noreferrer me" title=Instagram><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"/><path d="M16 11.37A4 4 0 1112.63 8 4 4 0 0116 11.37z"/><line x1="17.5" y1="6.5" x2="17.5" y2="6.5"/></svg>
</a><a href=https://www.linkedin.com/in/mayurhulke/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a><a href=https://www.reddit.com/user/LetsTalkWithRobots/ target=_blank rel="noopener noreferrer me" title=Reddit><svg viewBox="0 0 24 24" fill="currentcolor" stroke="none" stroke-width="1"><path d="M24 11.779c0-1.459-1.192-2.645-2.657-2.645-.715.0-1.363.286-1.84.746-1.81-1.191-4.259-1.949-6.971-2.046l1.483-4.669 4.016.941-.006.058c0 1.193.975 2.163 2.174 2.163 1.198.0 2.172-.97 2.172-2.163S21.396 2 20.199 2c-.92.0-1.704.574-2.021 1.379l-4.329-1.015c-.189-.046-.381.063-.44.249L11.755 7.82c-2.838.034-5.409.798-7.3 2.025-.474-.438-1.103-.712-1.799-.712-1.465.0-2.656 1.187-2.656 2.646.0.97.533 1.811 1.317 2.271-.052.282-.086.567-.086.857C1.231 18.818 6.039 22 11.95 22s10.72-3.182 10.72-7.093c0-.274-.029-.544-.075-.81.832-.447 1.405-1.312 1.405-2.318zM6.776 13.595c0-.868.71-1.575 1.582-1.575s1.581.707 1.581 1.575-.709 1.574-1.581 1.574-1.582-.706-1.582-1.574zm9.061 4.669c-.797.793-2.048 1.179-3.824 1.179L12 19.44l-.013.003c-1.777.0-3.028-.386-3.824-1.179-.145-.144-.145-.379.0-.523.145-.145.381-.145.526.0.65.647 1.729.961 3.298.961l.013.003.013-.003c1.569.0 2.648-.315 3.298-.962.145-.145.381-.144.526.0.145.145.145.379.0.524zm-.189-3.095c-.872.0-1.581-.706-1.581-1.574s.709-1.575 1.581-1.575 1.581.707 1.581 1.575-.709 1.574-1.581 1.574z"/></svg>
</a><a href=https://www.letstalkwithrobots.com/ target=_blank rel="noopener noreferrer me" title=Link><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71"/></svg></a></div></body></html>